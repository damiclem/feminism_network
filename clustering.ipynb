{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community\n",
    "import markov_clustering as markov\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import sparse\n",
    "from wordcloud import WordCloud\n",
    "from modules.network import get_degree\n",
    "\n",
    "# Get default colors\n",
    "colors = [*mcolors.TABLEAU_COLORS.values()]\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define periods under analysis\n",
    "years = [2017, 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionaries mapping indices to lemma tuples\n",
    "w2i = np.load('data/edges_w2i.npy', allow_pickle=True).item()\n",
    "i2w = np.load('data/edges_i2w.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges\n",
    "edges = pd.read_csv('data/database/edges.csv')\n",
    "edges = {y: edges[edges.year == y]['']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges 2017\n",
    "edges[2017].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges 2018\n",
    "edges[2018].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create undirected multigraph objects using edges table\n",
    "networks = {}\n",
    "for y in years:\n",
    "    networks[y] = nx.from_pandas_edgelist(edges[y], source='node_x', target='node_y', \n",
    "                                          edge_attr='counts', create_using=nx.MultiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors and labels for each POS tag\n",
    "pos_patches = {\n",
    "    'N': (colors[0], 'Noun'),  # Noun\n",
    "    'V': (colors[1], 'Verb'),  # Verb\n",
    "    'A': (colors[2], 'Adjective'),  # Adjective\n",
    "    'R': (colors[3], 'Adverb')   # Adverb\n",
    "}\n",
    "\n",
    "# Define function for plotting cluster wordcloud\n",
    "def make_word_cloud(word_cloud, scores):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - Lemmas: list of tuples (word text, pos tag)\n",
    "    - Scores: list of scores which define word importance\n",
    "    Note that len(scores) = len(lemmas) must be satisfied\n",
    "    Output:\n",
    "    - WordCloud object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use words only to define word positions\n",
    "    word_cloud = word_cloud.generate_from_frequencies(\n",
    "        {word_text: score for (word_text, pos_tag), score in scores.items()}\n",
    "    )\n",
    "    \n",
    "    # Modify layout\n",
    "    for i, (word, font_size, position, orientation, color) in enumerate(word_cloud.layout_):\n",
    "        # Get text and pos tag, separated\n",
    "        pos_tag = scores.index[i][1]\n",
    "        word_cloud.layout_[i] = (word, font_size, position, orientation, pos_patches[pos_tag][0])\n",
    "    \n",
    "    # Return modified wordcloud object\n",
    "    return word_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCL communities extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCL parameters\n",
    "\n",
    "# Inflaction: float > 1\n",
    "imin = 15       # minimum inflaction * 10\n",
    "imax = 40       # maximum inflaction * 10\n",
    "istep = 5       # step for inflaction exploration * 10\n",
    "\n",
    "# Expansion: integer > 1\n",
    "emin = 2        # minimum expansion \n",
    "emax = 8        # maximum expansion \n",
    "estep = 1       # step for expansion exploration\n",
    "\n",
    "max_iter = 500  # max number of iterations for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate modularity for each set of parameters\n",
    "\n",
    "# for k in adj_matrices.keys():\n",
    "#    # convert the matrix to sparse\n",
    "#    mat = sparse.csr_matrix(adj_matrices[k])\n",
    "#    print('{:s}\\'s network'.format(k))\n",
    "#    # Grid search for best parameters\n",
    "#    for inf in [i / 10 for i in range(imin, imax, istep)]:\n",
    "#        for exp in [e for e in range(emin, emax, estep)]:\n",
    "#            # Compute clusters\n",
    "#            result = markov.run_mcl(mat, pruning_threshold = 0, iterations = max_iter, inflation = inf, expansion = exp) \n",
    "#            clusters = markov.get_clusters(mat) \n",
    "#            print(\"Num. clusters:\",len(clusters))\n",
    "#            # Compute corresponding modularity\n",
    "#            Q = markov.modularity(matrix=result, clusters=clusters)\n",
    "#            print(\"inflation:\", inf, \"expansion:\", exp, \"modularity:\", Q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second evaluation: only on 2018 net\n",
    "#\n",
    "# print('\\n\\n2018\\'s network')\n",
    "# # convert the matrix to sparse\n",
    "# mat = sparse.csr_matrix(X[1])\n",
    "# exp = 2\n",
    "# inf_range = [40, 120]\n",
    "# for inf in [i/10 for i in range(inf_range[0], inf_range[1], 10)]:\n",
    "#    # Compute clusters\n",
    "#    result = markov.run_mcl(mat, pruning_threshold = 0, iterations = max_iter, inflation = inf, expansion = exp ) \n",
    "#    clusters = markov.get_clusters(mat) \n",
    "#    print(\"Num. clusters:\",len(clusters))\n",
    "#    # Compute corresponding modularity\n",
    "#    Q = markov.modularity(matrix=result, clusters=clusters)\n",
    "#    print(\"inflation:\", inf, \"expansion:\", exp, \"modularity:\", Q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "params = {\n",
    "    2017: {'inf': 1.5, 'exp': 4},  # mod = 0.0011712948963333333, n. clusters 48\n",
    "    2018: {'inf': 3.5, 'exp': 2}   # mod = 0.023 (?) , n. clusters 112\n",
    "}\n",
    "\n",
    "# Other parameters can be tried: \n",
    "# - 2018: higher inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute and print clusters\n",
    "\n",
    "mcl_clusters = {}\n",
    "\n",
    "for y in years:\n",
    "    # Retrieve adjacency matrix\n",
    "    adj_matrix = nx.to_numpy_matrix(networks[y], weight='counts')\n",
    "    # Convert the matrix to sparse\n",
    "    adj_matrix = sparse.csr_matrix(adj_matrix)\n",
    "    # MCL using best parameters\n",
    "    result = markov.run_mcl(adj_matrix, verbose = 1, pruning_threshold = 0, iterations = max_iter, \n",
    "                            inflation = params[y]['inf'], expansion = params[y]['exp']) \n",
    "    # Retrieve clusters according to dimension\n",
    "    clusters = sorted(markov.get_clusters(adj_matrix), key=len, reverse=True)\n",
    "    # Save clusters\n",
    "    mcl_clusters[y] = clusters\n",
    "    \n",
    "# Save clusters to disk\n",
    "np.save('data/mcl_clusters.npy', mcl_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCL communities analysis\n",
    "best = 20 \n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    net = networks[y]\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))    \n",
    "    axs = axs.ravel()\n",
    "    for j, c in enumerate(mcl_clusters[y][:4]):\n",
    "        degree = get_degree(nx.induced_subgraph(net, c)).sort_values(ascending=False).iloc[:best]        \n",
    "        _ = axs[j].bar(degree.index.map(lambda x: str(i2w[x])).values, degree.values)\n",
    "        _ = axs[j].tick_params(axis='x', labelrotation=60)\n",
    "    _ = plt.tight_layout()\n",
    "    _ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word clouds\n",
    "for i, y in enumerate(years):\n",
    "    # Get nets\n",
    "    net = networks[y]\n",
    "    # Define a 2x2 plot for each period (year) under analysis\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))    \n",
    "    axs = axs.ravel()\n",
    "    _ = fig.suptitle('Best 20 nodes for 4th most populated communities ({:d}) ranked by degree, descending'.format(y),\n",
    "                     size=16)\n",
    "    # Exctract WordCloud object from each cluster\n",
    "    for j, c in enumerate(mcl_clusters[y][:4]):\n",
    "        # Get degrees for current cluster\n",
    "        degree = get_degree(net.subgraph(c)).sort_values(ascending=False)\n",
    "        # Compute relative degree, needed for word scaling\n",
    "        degree = degree / degree.sum()\n",
    "        # Turn index from numeric to tuple (word text, pos_tag)\n",
    "        degree.index = degree.index.map(lambda i: i2w[i])\n",
    "        \n",
    "        # Define a WordCloud object using (word, POS tag) items\n",
    "        word_cloud = make_word_cloud(\n",
    "            WordCloud(max_font_size=50, background_color='white'),\n",
    "            degree\n",
    "        )\n",
    "        \n",
    "        # Make wordcloud plot\n",
    "        _ = axs[j].imshow(word_cloud, interpolation='bilinear')\n",
    "        _ = axs[j].set_axis_off()\n",
    "        _ = axs[j].legend(\n",
    "            handles=[mpatches.Patch(color=pos_patches[t][0], label=pos_patches[t][1]) for t in pos_patches.keys()], \n",
    "            loc='lower right'\n",
    "        )\n",
    "    \n",
    "    # Make plot\n",
    "    _ = plt.tight_layout()\n",
    "    _ = plt.subplots_adjust(top=0.95) \n",
    "    _ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louvain clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clusters for both nets\n",
    "lou_clusters = {y: pd.Series(community.best_partition(networks[y])) for y in years}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain communities analysis\n",
    "\n",
    "best = 20 \n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    # Initialize plot\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))    \n",
    "    axs = axs.ravel()\n",
    "    # Set title\n",
    "    _ = fig.suptitle('Best 20 nodes for 4 most populated communities ({:d}) ranked by degree, descending'.format(y))\n",
    "    # Extract 4 more populated clusters\n",
    "    clusters = partitions[y].groupby(by=partitions[y]).size().sort_values(ascending=False)\n",
    "    best_clusters = clusters.iloc[:4]\n",
    "    # Get nodes for each cluster\n",
    "    clusters = partitions[y][partitions[y].isin(best_clusters.index)]\n",
    "    clusters = clusters.groupby(by = clusters)\n",
    "    # Plot every cluster\n",
    "    for j, c in enumerate(clusters):\n",
    "        degree = get_degree(networks[y].subgraph(c[1].index.values)).sort_values(ascending=False).iloc[:best]        \n",
    "        _ = axs[j].bar(degree.index.map(lambda x: str(i2w[x])).values, degree.values)\n",
    "        _ = axs[j].tick_params(axis='x', labelrotation=60)\n",
    "    _ = plt.tight_layout()\n",
    "    _ = plt.subplots_adjust(top=0.95) \n",
    "    _ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum number of clusters and words to visualize for any cluster\n",
    "num_clusters = 4\n",
    "num_degrees = 20\n",
    "\n",
    "\n",
    "# Define a 2x2 plot for each period (year) under analysis\n",
    "fig, axs = plt.subplots(4, 2, figsize=(15, 15))    \n",
    "# Add title\n",
    "_ = fig.suptitle('Best four clusters, by size', size=16, y=1.04)\n",
    "# Add title on first plot left and first plot right\n",
    "_ = axs[0, 0].set_title('Clusters in 2017\\'s network')\n",
    "_ = axs[0, 1].set_title('Clusters in 2018\\'s network')\n",
    "\n",
    "# Plot word clouds\n",
    "for i, y in enumerate(networks.keys()):\n",
    "    # Get currently analyzed network object\n",
    "    net = networks[y]\n",
    "    \n",
    "    # Extract 4 more populated clusters\n",
    "    partitions_ = partitions[y]\n",
    "    clusters = partitions_.groupby(by=partitions_).size().sort_values(ascending=False)\n",
    "    best_clusters = clusters.index.values[:num_clusters]\n",
    "    \n",
    "    # Represent each cluster graphically\n",
    "    for j, c in enumerate(best_clusters):\n",
    "        \n",
    "        # Get current subgraph nodes\n",
    "        sub_nodes = nodes.index[nodes == c]\n",
    "        sub_graph = net.subgraph(sub_nodes)\n",
    "        \n",
    "        # Get degrees for current cluster\n",
    "        degree = get_degree(sub_graph).sort_values(ascending=False)\n",
    "        degree = degree / degree.sum() # Compute relative degree, needed for word scaling\n",
    "        # Turn index from numeric to tuple (word text, pos_tag)\n",
    "        degree.index = degree.index.map(lambda i: i2w[i])\n",
    "        \n",
    "        # Define a WordCloud object using (word, POS tag) items\n",
    "        word_cloud = make_word_cloud(WordCloud(max_font_size=50, background_color='white'), degree)\n",
    "        \n",
    "        # Make wordcloud plot\n",
    "        _ = axs[j, i].imshow(word_cloud, interpolation='bilinear')\n",
    "        _ = axs[j, i].set_axis_off()\n",
    "        _ = axs[j, i].legend(\n",
    "            handles=[mpatches.Patch(color=pos_patches[k][0], label=pos_patches[k][1]) for k in pos_patches.keys()], \n",
    "            loc='lower right'\n",
    "        )\n",
    "        \n",
    "# Make plot\n",
    "_ = plt.tight_layout(pad=0.5)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
